---
title: "P8105 - Homework 5"
author: "Linh Tran"
date: "11/12/2020"
output: github_document
---

```{r setup, include = FALSE}
library(tidyverse)
library(rvest)
library(ggridges)
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 12,
  fig.asp = .6,  
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom")) 

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_color_viridis_d
scale_colour_continuous = scale_color_viridis_c
```

# Problem 1

Read in the data

```{r, warning = FALSE, message = FALSE}
homicide_data = read_csv("homicide_data/homicide-data.csv")
```

The raw data has information on homicides in 50 large US cities gathered by the *Washington Post* , including victims' information (name, race, age, sex), location where the homicide occured (city, state) and disposition (closed/open with or without arrest). 

Create a `city_state` variable

```{r}
homicide_df = 
  homicide_data %>% 
  mutate(
    city_state = str_c(city, state, sep = "_"),
    resolved = case_when(
      disposition == "Closed without arrest"  ~ "unsolved",
      disposition == "Open/No arrest"         ~ "unsolved",
      disposition == "Closed by arrest"       ~ "solved",
    )
  ) %>% 
  select(city_state, resolved)
```

Summarize within cities to obtain the total number of homicides and the number of unsolved homicides.

```{r}
aggregate_df =
  homicide_df %>% 
  group_by(city_state) %>% 
  summarize(
     hom_total = n(),
     hom_unsolved = sum(resolved == "unsolved", na.rm = TRUE), 
  ) %>% 
  filter(city_state != "Tulsa_AL")   #Tulsa is in OK, not in AL
```


For the city of Baltimore, MD, use the prop.test function to estimate the proportion of homicides that are unsolved; save the output of prop.test as an R object, apply the broom::tidy to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.

```{r}
prop.test(
  aggregate_df %>%  filter(city_state == "Baltimore_MD") %>% pull(hom_unsolved),
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_total)
)  %>% 
  broom::tidy()
```

Try to iterate using map function

```{r}
results_df = 
  aggregate_df %>% 
  mutate(
    prop_tests = map2(.x = hom_unsolved, .y = hom_total, ~ prop.test(x = .x, n = .y)),
    tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
  )  %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high)
```

Plot showing the estimates and CIs for each city

```{r}
results_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust =1))
```

Another solution

```{r, error = TRUE}
city_prop_test = function(df) {
  n_unsolved...
  n_total...
  prop.test(....)
}
```


# Problem 2

## Tidy dataframe

**Tidy dataframe containing data from all participants (subject ID, arm, observations over time)**

```{r, warning = FALSE, message = FALSE}
# Import one dataset
data_1 = read_csv("lda_data/con_01.csv")

# Import all dataset
lda_df = 
  tibble(
    path = list.files("lda_data") 
  ) %>% 
  mutate(
    path = str_c("lda_data/", path),
    data = map(path, read_csv),
    path = str_replace(path, "^lda_data/", ""),
    path = str_replace(path, ".csv$", "")
  ) %>% 
  separate(path, into = c("arm", "subject_id"), sep = "_") %>% 
  unnest(data) %>% 
  pivot_longer(week_1:week_8,
               names_to = "week",
               values_to = "observation"
  ) %>% 
  mutate(
    week = as.numeric(str_replace(week, "^week_", "")),
    arm = recode_factor(arm,"con" = "control", "exp" = "experiment")
    ) 

lda_df
```

## Spaghetti plot

**A spaghetti plot showing observations on each subject over time, and comment on differences between groups.**

```{r}
lda_df %>% 
  ggplot(aes(x = week, y = observation, color = subject_id, linetype = arm)) +
  geom_line() +
  labs(x = "Week",
       y = "Observation",
       title = "Observations for each subject over eight week period")
```

**Comment**: Observations for the control group and experimental group have roughly similar entry points. However, over time observations for subjects in the experimental arm steadily increases while the subjects from the control arms stay fairly constant over the eight week period. In general, at the end week 8, the experimental arm has higher observed values than control arm.

# Problem 3

## Simulate data

**Set μ = 0. Generate 5000 datasets from x ~ Normal[μ,σ]**

```{r}
set.seed(1)

sim_ttest = function(mu) {
  
  sim_data = tibble(
    x = rnorm(n = 30, mean = mu, sd = 5),
  )
  
  sim_data %>% 
  t.test(mu = 0, alternative = "two.sided", conf.level = 0.95) %>% 
    broom::tidy() %>% 
    select(estimate, p.value)
}

# 5000 datasets for mu = 0
sim_ttest_0 = 
  rerun(5000, sim_ttest(0)) %>% 
  bind_rows()
```

**Repeat the above for μ={1,2,3,4,5,6}**

```{r}
sim_results = 
  tibble(
  mu = c(0:6)
) %>% 
  mutate(
    output = map(.x = mu, ~rerun(5000, sim_ttest(mu = .x))),
    results = map(output, bind_rows)
    ) %>% 
  select(-output) %>% 
  unnest(results)

sim_results 
```


## Plots

### Plot 1

**Plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ on the x axis**

```{r}
decision_df =
  sim_results %>% 
  mutate(
    decision = case_when(
      p.value < 0.05 ~ "reject",
      p.value >= 0.05 ~ "fail to reject"
    )
  ) %>% 
  group_by(mu) %>% 
  summarize(
    decision_total = n(),
    decision_reject = sum(decision == "reject")
  ) %>% 
  mutate(
    prop_tests = map2(.x = decision_reject, .y = decision_total, ~prop.test(x = .x, n = .y)),
    tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
  ) %>% 
  unnest(tidy_tests)

decision_df %>%
  ggplot(aes(x = mu, y = estimate)) +
  geom_point() +
  geom_smooth(se = FALSE) +
  labs(
    x = "μ",
    y = "Power (proportion of times the null was rejected)",
    title = "The Association between Effect Size and Power"
  )
```

**Comment on plot 1**: As the effect size increases, the power increases. Under the null hypothesis of mu = 0, as the effect size (mu) increases from 1 to 6, the power to reject the null hypothesis also increases because the difference between the null mu value (0) and the true value of mu increases. 

### Plot 2

**Plot showing the average estimate of μ̂  on the y axis and the true value of μ on the x axis **

```{r, fig.width = 10, fig.height = 6}
decision_all =
  sim_results %>% 
  group_by(mu) %>% 
  summarize(all_decision = mean(estimate))

decision_reject =
  sim_results %>% 
  filter(p.value < 0.05) %>% 
  group_by(mu) %>% 
  summarize(rejected_decision = mean(estimate))

# Combine plots

left_join(decision_all, decision_reject, by = "mu") %>%
  pivot_longer(
    all_decision:rejected_decision,
    names_to = "types",
    values_to = "average_estimated_mu"
  ) %>% 
  ggplot(aes(x = mu, y = average_estimated_mu, color = types)) +
  geom_point() +
  geom_smooth(se = FALSE) +
  labs(
    x = "True value of mu",
    y = "Average estimated value of mu",
    title = "Comparison between true value of μ and the average estimate of μ̂"
  )
```

**Comment on plot 2**: The average estimated value of mu for tests in which the null was rejected are higher than the true value of mu and then become approximately equal to true value of mu for mu >= 3. This makes sense because the power of the test increases as the effect size increases, at a true mu of 4-6, the rejected mu hat is equal to the average mu hat in all decision because we would have a 100% rejection rate.


